#!/usr/bin/env python3
"""
ì›¹ í˜ì´ì§€ í‚¤ì›Œë“œ í•„í„°ë§ íˆ´
URL ëª©ë¡ê³¼ í‚¤ì›Œë“œ ëª©ë¡ì„ íŒŒì¼ì—ì„œ ì½ì–´ì™€ì„œ ë§¤ì¹­ë˜ëŠ” ì¤„ì„ ì¶œë ¥í•©ë‹ˆë‹¤.
"""

import re
import sys
import argparse
import requests
import pandas as pd
from typing import List, Tuple
from urllib.parse import urlparse
from html import unescape


class WebLineFilter:
    def __init__(self, urls_file: str, keywords_file: str):
        """
        ì´ˆê¸°í™”
        
        Args:
            urls_file: URL ëª©ë¡ì´ ì €ì¥ëœ íŒŒì¼ ê²½ë¡œ
            keywords_file: í‚¤ì›Œë“œ ëª©ë¡ì´ ì €ì¥ëœ íŒŒì¼ ê²½ë¡œ
        """
        self.urls = self._load_urls(urls_file)
        self.keywords = self._load_keywords(keywords_file)
        
    def _load_urls(self, filepath: str) -> List[str]:
        """URL íŒŒì¼ì—ì„œ URL ëª©ë¡ ë¡œë“œ"""
        try:
            with open(filepath, 'r', encoding='utf-8') as f:
                urls = [line.strip() for line in f if line.strip() and not line.strip().startswith('#')]
            print(f"âœ“ {len(urls)}ê°œì˜ URLì„ ë¡œë“œí–ˆìŠµë‹ˆë‹¤.")
            return urls
        except FileNotFoundError:
            print(f"âŒ ì˜¤ë¥˜: '{filepath}' íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            sys.exit(1)
        except Exception as e:
            print(f"âŒ URL íŒŒì¼ ì½ê¸° ì˜¤ë¥˜: {e}")
            sys.exit(1)
    
    def _load_keywords(self, filepath: str) -> List[Tuple[str, bool, re.Pattern]]:
        """
        í‚¤ì›Œë“œ íŒŒì¼ì—ì„œ í‚¤ì›Œë“œ ë¡œë“œ
        
        Returns:
            List of (ì›ë³¸ í‚¤ì›Œë“œ, ì •ê·œì‹ ì—¬ë¶€, íŒ¨í„´) íŠœí”Œ
        """
        try:
            keywords = []
            with open(filepath, 'r', encoding='utf-8') as f:
                for line in f:
                    line = line.strip()
                    if not line or line.startswith('#'):
                        continue
                    
                    # ì •ê·œì‹ íŒ¨í„´ í™•ì¸
                    if line.startswith('<<REGEX>>'):
                        regex_pattern = line[9:].strip()
                        try:
                            compiled_pattern = re.compile(regex_pattern, re.IGNORECASE)
                            keywords.append((regex_pattern, True, compiled_pattern))
                        except re.error as e:
                            print(f"âš ï¸  ê²½ê³ : ì˜ëª»ëœ ì •ê·œì‹ '{regex_pattern}': {e}")
                            continue
                    else:
                        # ì¼ë°˜ ë¬¸ìì—´ ê²€ìƒ‰ (ëŒ€ì†Œë¬¸ì ë¬´ì‹œ)
                        pattern = re.compile(re.escape(line), re.IGNORECASE)
                        keywords.append((line, False, pattern))
            
            print(f"âœ“ {len(keywords)}ê°œì˜ í‚¤ì›Œë“œë¥¼ ë¡œë“œí–ˆìŠµë‹ˆë‹¤.")
            return keywords
        except FileNotFoundError:
            print(f"âŒ ì˜¤ë¥˜: '{filepath}' íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            sys.exit(1)
        except Exception as e:
            print(f"âŒ í‚¤ì›Œë“œ íŒŒì¼ ì½ê¸° ì˜¤ë¥˜: {e}")
            sys.exit(1)
    
    def _clean_html(self, text: str) -> str:
        """
        HTML íƒœê·¸ë¥¼ ì œê±°í•˜ê³  <br> íƒœê·¸ë¥¼ ê°œí–‰ìœ¼ë¡œ ë³€í™˜
        
        Args:
            text: ì›ë³¸ HTML í…ìŠ¤íŠ¸
            
        Returns:
            ì •ë¦¬ëœ í…ìŠ¤íŠ¸
        """
        # <br>, <br/>, <br /> íƒœê·¸ë¥¼ ê°œí–‰ìœ¼ë¡œ ë³€í™˜ (ëŒ€ì†Œë¬¸ì ë¬´ì‹œ)
        text = re.sub(r'<br\s*/?>', '\n', text, flags=re.IGNORECASE)
        
        # ëª¨ë“  HTML íƒœê·¸ ì œê±°
        text = re.sub(r'<[^>]+>', '', text)
        
        # HTML ì—”í‹°í‹° ë””ì½”ë”© (&nbsp;, &lt; ë“±)
        text = unescape(text)
        
        return text
    
    def _fetch_webpage(self, url: str) -> Tuple[str, List[str]]:
        """
        ì›¹ í˜ì´ì§€ë¥¼ ê°€ì ¸ì™€ì„œ HTML íƒœê·¸ë¥¼ ì œê±°í•˜ê³  ì¤„ ë‹¨ìœ„ë¡œ ë¶„ë¦¬
        
        Returns:
            (URL, ì¤„ ëª©ë¡) íŠœí”Œ
        """
        try:
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
            }
            response = requests.get(url, headers=headers, timeout=10)
            response.raise_for_status()
            
            # HTML íƒœê·¸ ì œê±° ë° <br> íƒœê·¸ë¥¼ ê°œí–‰ìœ¼ë¡œ ë³€í™˜
            cleaned_text = self._clean_html(response.text)
            
            # í…ìŠ¤íŠ¸ë¥¼ ì¤„ ë‹¨ìœ„ë¡œ ë¶„ë¦¬
            lines = cleaned_text.split('\n')
            return url, lines
        except requests.exceptions.RequestException as e:
            print(f"âš ï¸  URL '{url}' ê°€ì ¸ì˜¤ê¸° ì‹¤íŒ¨: {e}")
            return url, []
    
    def _search_lines(self, url: str, lines: List[str]) -> List[Tuple[int, str, str]]:
        """
        ì¤„ì—ì„œ í‚¤ì›Œë“œ ê²€ìƒ‰
        
        Returns:
            List of (ì¤„ ë²ˆí˜¸, ë§¤ì¹­ëœ í‚¤ì›Œë“œ, ì¤„ ë‚´ìš©) íŠœí”Œ
        """
        matches = []
        for line_num, line in enumerate(lines, 1):
            line_stripped = line.strip()
            if not line_stripped:
                continue
            
            for keyword, is_regex, pattern in self.keywords:
                if pattern.search(line):
                    matches.append((line_num, keyword, line_stripped))
                    break  # í•˜ë‚˜ì˜ ì¤„ì— ëŒ€í•´ ì²« ë²ˆì§¸ ë§¤ì¹­ë§Œ ê¸°ë¡
        
        return matches
    
    def run(self, output_file: str = None, csv_file: str = None):
        """
        ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜
        
        Args:
            output_file: ê²°ê³¼ë¥¼ ì €ì¥í•  í…ìŠ¤íŠ¸ íŒŒì¼ ê²½ë¡œ (ì„ íƒì‚¬í•­)
            csv_file: ê²°ê³¼ë¥¼ ì €ì¥í•  CSV íŒŒì¼ ê²½ë¡œ (ì„ íƒì‚¬í•­)
        """
        print("\n" + "=" * 80)
        print("ì›¹ í˜ì´ì§€ í‚¤ì›Œë“œ í•„í„°ë§ ì‹œì‘")
        print("=" * 80 + "\n")
        
        all_results = []
        
        for idx, url in enumerate(self.urls, 1):
            print(f"[{idx}/{len(self.urls)}] ì²˜ë¦¬ ì¤‘: {url}")
            
            url, lines = self._fetch_webpage(url)
            if not lines:
                continue
            
            matches = self._search_lines(url, lines)
            
            if matches:
                print(f"  âœ“ {len(matches)}ê°œì˜ ë§¤ì¹­ ë°œê²¬\n")
                all_results.append((url, matches))
            else:
                print(f"  - ë§¤ì¹­ ì—†ìŒ\n")
        
        # ê²°ê³¼ ì¶œë ¥
        self._display_results(all_results)
        
        # íŒŒì¼ë¡œ ì €ì¥ (ì„ íƒì‚¬í•­)
        if output_file:
            self._save_results(all_results, output_file)
        
        # CSV íŒŒì¼ë¡œ ì €ì¥ (ì„ íƒì‚¬í•­)
        if csv_file:
            self._save_results_to_csv(all_results, csv_file)
    
    def _display_results(self, results: List[Tuple[str, List[Tuple[int, str, str]]]]):
        """ê²°ê³¼ë¥¼ ì½˜ì†”ì— ì¶œë ¥"""
        print("\n" + "=" * 80)
        print("ê²€ìƒ‰ ê²°ê³¼")
        print("=" * 80 + "\n")
        
        if not results:
            print("ë§¤ì¹­ë˜ëŠ” ë‚´ìš©ì´ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        total_matches = 0
        for url, matches in results:
            total_matches += len(matches)
            print(f"\nğŸ“„ URL: {url}")
            print(f"   ë§¤ì¹­ ìˆ˜: {len(matches)}")
            print("-" * 80)
            
            for line_num, keyword, line in matches:
                # í‚¤ì›Œë“œ í‘œì‹œ
                keyword_display = f"[{keyword}]" if len(keyword) < 30 else f"[{keyword[:27]}...]"
                # ì¤„ ë²ˆí˜¸ì™€ í‚¤ì›Œë“œ í‘œì‹œ
                print(f"  ì¤„ {line_num:5d} | í‚¤ì›Œë“œ: {keyword_display}")
                # ë§¤ì¹­ëœ ì¤„ ì „ì²´ ì¶œë ¥
                print(f"  ë‚´ìš©: {line}")
                print()
        
        print("\n" + "=" * 80)
        print(f"ì´ {len(results)}ê°œ URLì—ì„œ {total_matches}ê°œì˜ ë§¤ì¹­ì„ ë°œê²¬í–ˆìŠµë‹ˆë‹¤.")
        print("=" * 80)
    
    def _save_results(self, results: List[Tuple[str, List[Tuple[int, str, str]]]], filepath: str):
        """ê²°ê³¼ë¥¼ íŒŒì¼ë¡œ ì €ì¥"""
        try:
            with open(filepath, 'w', encoding='utf-8') as f:
                f.write("ì›¹ í˜ì´ì§€ í‚¤ì›Œë“œ í•„í„°ë§ ê²°ê³¼\n")
                f.write("=" * 80 + "\n\n")
                
                for url, matches in results:
                    f.write(f"URL: {url}\n")
                    f.write(f"ë§¤ì¹­ ìˆ˜: {len(matches)}\n")
                    f.write("-" * 80 + "\n")
                    
                    for line_num, keyword, line in matches:
                        f.write(f"ì¤„ {line_num} | í‚¤ì›Œë“œ: {keyword}\n")
                        f.write(f"ë‚´ìš©: {line}\n\n")
                    
                    f.write("\n")
            
            print(f"\nâœ“ ê²°ê³¼ê°€ '{filepath}'ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
        except Exception as e:
            print(f"âš ï¸  íŒŒì¼ ì €ì¥ ì˜¤ë¥˜: {e}")
    
    def _save_results_to_csv(self, results: List[Tuple[str, List[Tuple[int, str, str]]]], filepath: str):
        """ê²°ê³¼ë¥¼ CSV íŒŒì¼ë¡œ ì €ì¥ (pandas DataFrame ì‚¬ìš©)"""
        try:
            # DataFrameìš© ë°ì´í„° ì¤€ë¹„
            data = []
            for url, matches in results:
                for line_num, keyword, line in matches:
                    data.append({
                        'URL': url,
                        'ì¤„ë²ˆí˜¸': line_num,
                        'í‚¤ì›Œë“œ': keyword,
                        'ë§¤ì¹­ë‚´ìš©': line
                    })
            
            # DataFrame ìƒì„±
            if data:
                df = pd.DataFrame(data)
                
                # CSV íŒŒì¼ë¡œ ì €ì¥
                df.to_csv(filepath, index=False, encoding='utf-8-sig')  # utf-8-sig: Excel í˜¸í™˜ì„±
                
                print(f"\nâœ“ CSV ê²°ê³¼ê°€ '{filepath}'ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
                print(f"  ì´ {len(df)}ê°œì˜ ë§¤ì¹­ì´ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
            else:
                print(f"\nâš ï¸  ì €ì¥í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        except Exception as e:
            print(f"âš ï¸  CSV íŒŒì¼ ì €ì¥ ì˜¤ë¥˜: {e}")


def main():
    parser = argparse.ArgumentParser(
        description='ì›¹ í˜ì´ì§€ì—ì„œ í‚¤ì›Œë“œë¥¼ ê²€ìƒ‰í•˜ì—¬ ë§¤ì¹­ë˜ëŠ” ì¤„ì„ í•„í„°ë§í•©ë‹ˆë‹¤.',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ì‚¬ìš© ì˜ˆì œ:
  python web_filter.py urls.txt keywords.txt
  python web_filter.py urls.txt keywords.txt -o results.txt
  python web_filter.py urls.txt keywords.txt -c results.csv
  python web_filter.py urls.txt keywords.txt -o results.txt -c results.csv

íŒŒì¼ í˜•ì‹:
  urls.txt      - í•œ ì¤„ì— í•˜ë‚˜ì˜ URL
  keywords.txt  - í•œ ì¤„ì— í•˜ë‚˜ì˜ í‚¤ì›Œë“œ
                  ì •ê·œì‹ì˜ ê²½ìš° ì•ì— <<REGEX>> ì¶”ê°€
                  ì˜ˆ: <<REGEX>>\\d{3}-\\d{4}

ì£¼ì„:
  '#'ë¡œ ì‹œì‘í•˜ëŠ” ì¤„ì€ ë¬´ì‹œë©ë‹ˆë‹¤.
        """
    )
    
    parser.add_argument('urls_file', help='URL ëª©ë¡ì´ ì €ì¥ëœ íŒŒì¼')
    parser.add_argument('keywords_file', help='í‚¤ì›Œë“œ ëª©ë¡ì´ ì €ì¥ëœ íŒŒì¼')
    parser.add_argument('-o', '--output', help='ê²°ê³¼ë¥¼ ì €ì¥í•  í…ìŠ¤íŠ¸ íŒŒì¼ ê²½ë¡œ', default=None)
    parser.add_argument('-c', '--csv', help='ê²°ê³¼ë¥¼ ì €ì¥í•  CSV íŒŒì¼ ê²½ë¡œ', default=None)
    
    args = parser.parse_args()
    
    # í•„í„° ì‹¤í–‰
    filter_tool = WebLineFilter(args.urls_file, args.keywords_file)
    filter_tool.run(args.output, args.csv)


if __name__ == '__main__':
    main()
